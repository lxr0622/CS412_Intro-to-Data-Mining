# CS412_Intro-to-Data-Mining Project


### Executive Summary
For prudential life insurance project on Kaggle, after preprocessing steps, 3 different algorithms were implemented. Naïve Bayes, ADABoost and Linear Discriminant Analysis. From those 3, ADABoost used sklearn packages and therefore was not used for submission but it was only used for comparison. Kaggle scores of the three methods are 0.41, 0.49, 0.51 respectively the best result corresponding to LDA. Therefore LDA was used to classify the dataset.

### Project Introduction
In real world, insurance companies are prudent in conducting risk analysis on applications before issuing the insurance. It is obvious that the applications with lower risk are more likely to successfully gain full coverage of insurance for goods or applicants. Thus, the insurance companies need to establish evaluation models on how to get the highest possible profits among all the applications based on their risk levels and at the same time, the clients can have ideas on how to build up the possibilities of being covered based on the evaluation criteria.
However, it becomes a tough process when many factors are taken into consideration. And not all of the factors can be represented as meaningful numeric numbers that can be interpreted in the evaluation model. For example, car brand, car model, car year, car accident history and so on, are required when applying car insurance, but it is difficult to represent car model by a meaningful value and different car brand has different car models, which makes it even harder to fill in this attribute. Besides, the real world dataset can have incomplete data, noisy data, inconsistent data and intentional data due to faulty instruments, human or computer error, transmission error, etc.
In this project, the main task is to learn to predict the risk level of existing clients based on 20000 training client dataset and test the prediction in 10000 client dataset. Each record is provided with 127 different attributes that can possibly contribute to risk level. This prediction is important for the insurance company to filtrate those applicants with lower risk level so that they can have higher promising profits. But the challenges occur that there are 127 attributes that can impact the risk level results, which is a lot. And many data are missing in the dataset which makes the dataset incomplete. What’s more, this is a multi-classification problem, which means it is more complicated to build the classifiers than binary classification problem. And in the large training dataset, it is easy to observe the highly in-equal size of different classes, which will make it very hard to predict results. Without using existing code package, it will also take longer time to code the different algorithms to train and predict the dataset.

### Analysis
1. To solve challenge mentioned above, for continuous attributes, mean values are used to fill the missing data and for the other attributes, modes values are used.
2. Since the 10000 testing dataset records does not have the risk level information, our group firstly randomly take 85% of 20000 training dataset as training set to gain the prediction model using several different algorithms: Linear Discriminate Analysis(LDA) and Naive Bayes (NB). And then the rest 15% of 20000 dataset are used as testing set to roughly evaluate the accuracy. It can briefly provide an intuitive evaluation on those algorithms.
3. Finally, our group apply these algorithms on all the 10000 testing dataset and get scores on Kaggle. The highest score 0.51 we get is using LDA (See LDA.py) and 0.41 is using NB (See trainingNB.py and testingNB.py).
4. Since ensemble method can help increase the accuracy of base classifiers, Logistic Regression (LR) as base classifier was coded but failed to do Adaboost since it has accuracy lower than 0.5 (See Adaboost_LR_failed.py). But if using the sklearn logistic regression package to do Adaboost, the code can run three rounds to get a score of 0.496 in Kaggle, but it cannot run more rounds since the accuracy can hardly exceed 0.5 from 4th round (See Adaboost_LR_sklearn.py)
